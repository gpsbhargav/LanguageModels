{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dill as pickle\n",
    "import os\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "from random import uniform\n",
    "from math import floor\n",
    "import shutil\n",
    "import codecs\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "#from nltk.corpus import words\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wordlists\n",
      "\n",
      "en: English, http://en.wikipedia.org/wiki/Words_(Unix)\n",
      "en-basic: 850 English words: C.K. Ogden in The ABC of Basic English (1932)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(words.readme())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of words in each dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_brown ='../data/brown/brown_all/'\n",
    "PATH_gutenberg ='../data/gutenberg/gutenberg_all/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of words in Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1005088\r\n"
     ]
    }
   ],
   "source": [
    "!find {PATH_brown} -name '*.*' | xargs cat | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of words in Gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2102546\r\n"
     ]
    }
   ],
   "source": [
    "!find {PATH_gutenberg} -name '*.*' | xargs cat | wc -w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetSplitter:\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __get_file_list_from_dir(self,datadir):\n",
    "        all_files = os.listdir(os.path.abspath(datadir))\n",
    "        data_files = list(filter(lambda file: file.endswith('.txt'), all_files))\n",
    "        return data_files\n",
    "    \n",
    "    def __copy_files(self,src_path,target_path,file_list):\n",
    "        for file_name in file_list:\n",
    "            full_file_name = os.path.join(src_path, file_name)\n",
    "            if (os.path.isfile(full_file_name)):\n",
    "                shutil.copy(full_file_name, target_path)\n",
    "        \n",
    "    def __make_dir(self,directory):\n",
    "        try:\n",
    "            os.makedirs(directory)\n",
    "            return True\n",
    "        except OSError as e:\n",
    "            if e.errno != errno.EEXIST:\n",
    "                return False\n",
    "        \n",
    "    def get_train_and_test_sets(self,datadir,percentage_train=0.9):\n",
    "        file_list = self.__get_file_list_from_dir(datadir)\n",
    "        shuffle(file_list)\n",
    "        split = percentage_train\n",
    "        split_index = floor(len(file_list) * split)\n",
    "        training = file_list[:split_index]\n",
    "        testing = file_list[split_index:]\n",
    "        return training, testing \n",
    "    \n",
    "    def get_train_validate_test_sets(self,datadir,percentage_train=0.9,percentage_validate=0.1):\n",
    "        train,test = self.get_train_and_test_sets(datadir,percentage_train=percentage_train)\n",
    "        split = 1-percentage_validate\n",
    "        split_index = floor(len(train) * split)\n",
    "        validate = train[split_index:]\n",
    "        train = train[:split_index]\n",
    "        return train,validate,test\n",
    "\n",
    "    def copy_to_dirs(self,src_path,target_path,train,validate,test):\n",
    "        if self.__make_dir(target_path+\"train\"):\n",
    "            self.__copy_files(src_path,target_path+\"train/\",train)\n",
    "        if self.__make_dir(target_path+\"validate\"):\n",
    "            self.__copy_files(src_path,target_path+\"validate/\",validate)\n",
    "        if self.__make_dir(target_path+\"test\"):\n",
    "            self.__copy_files(src_path,target_path+\"test/\",test)\n",
    "            \n",
    "\n",
    "#data_splitter = DatasetSplitter()\n",
    "#PATH = PATH_brown\n",
    "#test,validate,train = data_splitter.get_train_validate_test_sets(PATH)\n",
    "#data_splitter.copy_to_dirs(PATH,PATH+\"../\",test,validate,train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../data/brown/train/train.txt'\n",
    "TEST_PATH = '../data/brown/validate/validate.txt'\n",
    "\n",
    "#TRAIN_PATH = '../data/brown/sample/good_sent.txt'\n",
    "#TEST_PATH = '../data/brown/sample/good_sent.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "f = codecs.open(TRAIN_PATH, encoding='utf-8')\n",
    "tokens = nltk.word_tokenize(f.read())\n",
    "tokens_original = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for token in tokens:\n",
    "    # print(token)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bigrams = nltk.ngrams(['a','b'],2)\n",
    "type(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# replace \"few\" rare words with < unk >  (no spaces inside the unk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unigram_counter = Counter(ngrams(tokens,1))\n",
    "tuple([\"it\",'is'])\n",
    "#unigram_counter[('it',)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert_to_unk = []\n",
    "for item in unigram_counter.keys():\n",
    "    if unigram_counter[item] < 2:\n",
    "        convert_to_unk.append(item[0])\n",
    "num_unks_needed = max(floor(0.01 * unigram_counter.most_common(1)[0][1]),2)\n",
    "convert_to_unk = convert_to_unk[:num_unks_needed]\n",
    "for i in range(len(tokens_original)):\n",
    "    if(tokens_original[i] in convert_to_unk):\n",
    "        tokens[i] = '<unk>'\n",
    "\n",
    "unigram_counter = Counter(ngrams(tokens,1))\n",
    "print(unigram_counter[('<unk>',)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNLM:\n",
    "    def __init__(self,order=2):\n",
    "        self.order = 2\n",
    "    \n",
    "    def set_training_data(self,train_file):\n",
    "        '''\n",
    "        reads file, tokenizes, replaces \"some\" rare words with <unk>. stores these in the instance \n",
    "        variable tokens\n",
    "        '''\n",
    "        self.train_file = train_file\n",
    "        f = codecs.open(self.train_file, encoding='utf-8')\n",
    "        tokens = nltk.word_tokenize(f.read())\n",
    "        f.close()\n",
    "        tokens_original = tokens\n",
    "        unigram_counter = Counter(ngrams(tokens,1))\n",
    "#        convert_to_unk = []\n",
    "#        for item in unigram_counter.keys():\n",
    "#            if unigram_counter[item] < 2:\n",
    "#                convert_to_unk.append(item[0])               \n",
    "#        num_unks_needed = max(floor(1 * unigram_counter.most_common(1)[0][1]),2)\n",
    "#        convert_to_unk = convert_to_unk[:num_unks_needed]\n",
    "#        for i in range(len(tokens_original)):\n",
    "#            if(tokens_original[i] in convert_to_unk):\n",
    "#                tokens[i] = '<unk>'\n",
    "#        print(\"Num unk={}\".format(num_unks_needed))\n",
    "        \n",
    "        num_unk = 0\n",
    "        for i in range(len(tokens_original)):\n",
    "            if(  unigram_counter[tuple([tokens_original[i]])] < 3):\n",
    "                tokens[i] = '<unk>'\n",
    "                num_unk = num_unk + 1\n",
    "        print(\"Num unk={}\".format(num_unk))\n",
    "        \n",
    "        \n",
    "        self.tokens = [\"<pad>\"] + tokens\n",
    "#        self.text = nltk.Text(self.tokens)\n",
    "#        self.token_searcher = nltk.TokenSearcher(self.tokens)\n",
    "        self.unigram_counter = Counter(ngrams(self.tokens,1))\n",
    "        self.bigram_counter = Counter(ngrams(self.tokens,2))\n",
    "#        self.vocabulary = self.text.vocab()\n",
    "        self.vocabulary = set(self.tokens)\n",
    "        self.vocabulary_list = list(self.vocabulary)\n",
    "        #print(unigram_counter[('<unk>',)])\n",
    "        \n",
    "    def train(self):\n",
    "        print(\"Training : Getting counts from training data\")\n",
    "        self.unique_continuations = defaultdict(set)\n",
    "        self.unique_contexts = defaultdict(set)\n",
    "        for bigram in tqdm(self.bigram_counter.keys(),total=len(self.bigram_counter.keys())):\n",
    "            self.unique_continuations[bigram[0]].add(bigram[1])\n",
    "            self.unique_contexts[bigram[1]].add(bigram[0])\n",
    "    \n",
    "    def get_log_prob_kn(self,ngram):\n",
    "        abs_discount = 0.85\n",
    "        count_input_ngram = max(self.bigram_counter[ngram] - abs_discount , 0)\n",
    "        count_context = self.unigram_counter[tuple([ngram[0]])]\n",
    "        num_unique_continuations = len(self.unique_continuations[ngram[0]])\n",
    "        num_unique_contexts = len(self.unique_continuations[ngram[1]])\n",
    "#        for bigram in self.bigram_counter.keys():\n",
    "#            if(bigram[0] == ngram[0]):\n",
    "#                unique_continuations.add(bigram[-1])    \n",
    "#            if(bigram[-1] == ngram[-1]):\n",
    "#                unique_contexts.add(bigram[0])\n",
    "#        print(\"ngram:{}, count;{}\".format(tuple([ngram[0]]),count_context))\n",
    "        interpolation_weight = (abs_discount / count_context) * num_unique_continuations\n",
    "        continuation_probability = num_unique_contexts / len(self.bigram_counter)\n",
    "        p_kn = (count_input_ngram / count_context) + (interpolation_weight * continuation_probability)\n",
    "        #return math.log(p_kn)\n",
    "        return p_kn\n",
    "    \n",
    "    def __preproc_test_input(self,tokens):\n",
    "        '''converts words not in vocab to <unk>'''\n",
    "        print(\"preprocessing training data\")\n",
    "        unk_count = 0\n",
    "        for i in range(len(tokens)):\n",
    "            if(tokens[i] not in  self.vocabulary):\n",
    "                tokens[i] = '<unk>'\n",
    "                unk_count = unk_count + 1\n",
    "        tokens = ['<pad>'] + tokens\n",
    "        print(\"data length={}, number of <unk>={}\".format(len(tokens),unk_count))\n",
    "        return tokens\n",
    "    \n",
    "    def get_perplexity(self,test_file):\n",
    "        f = codecs.open(test_file, encoding='utf-8')\n",
    "        tokens = nltk.word_tokenize(f.read())\n",
    "        tokens = self.__preproc_test_input(tokens)\n",
    "        f.close()\n",
    "        bigrams = nltk.ngrams(tokens,2)\n",
    "        sum_log_prob = 0\n",
    "        print(\"Calculating perplexity\")\n",
    "        iter_count = 0\n",
    "        for bigram in tqdm(bigrams,total=len(tokens)-1): \n",
    "            prob = math.log(self.get_log_prob_kn(bigram))\n",
    "            sum_log_prob = sum_log_prob + prob\n",
    "            iter_count += 1\n",
    "        print(\"Num iterations = {}\".format(iter_count))\n",
    "        return math.exp( -(1.0/len(tokens)) * sum_log_prob )\n",
    "    \n",
    "    def get_random_word(self):\n",
    "        index = randint(0,len(self.vocabulary_list)-1)\n",
    "        return kn_lm.vocabulary_list[index]\n",
    "\n",
    "    \n",
    "    def generate_text(self,num_tokens=10):\n",
    "        sentence = ['<pad>']\n",
    "        for i in range(num_tokens+1):\n",
    "            random_word = self.get_random_word()\n",
    "            bigram = tuple(sentence[-1:] + [random_word])\n",
    "            #prob_bigram = math.exp(self.get_log_prob_kn(bigram))\n",
    "            prob_bigram = self.get_log_prob_kn(bigram)\n",
    "            #print(\"bigram={}, prob={}\".format(bigram,prob_bigram))\n",
    "            random_number = uniform(0,1)\n",
    "            if(random_number < prob_bigram):\n",
    "                sentence = sentence + [random_word]\n",
    "        return ' '.join(sentence[1:])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 10.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kn_lm = KNLM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num unk=29786\n",
      "CPU times: user 6.19 s, sys: 32 ms, total: 6.22 s\n",
      "Wall time: 6.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kn_lm.set_training_data(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 70413/331239 [00:00<00:00, 703971.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training : Getting counts from training data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 331239/331239 [00:00<00:00, 461111.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 708 ms, sys: 16 ms, total: 724 ms\n",
      "Wall time: 720 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "kn_lm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 26649/90646 [00:00<00:00, 266443.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing training data\n",
      "data length=90647, number of <unk>=5977\n",
      "Calculating perplexity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 90646/90646 [00:00<00:00, 267576.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num iterations = 90646\n",
      "CPU times: user 780 ms, sys: 4 ms, total: 784 ms\n",
      "Wall time: 780 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "416.2034752846155"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "kn_lm.get_perplexity(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_lm.generate_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'he could be no you and destroy the membership established meticulously blue place redoute lot of the only the'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = ['<pad>']\n",
    "while(len(sentence) < 20):\n",
    "    random_word = kn_lm.get_random_word()\n",
    "    if(random_word == '<unk>'):\n",
    "        continue\n",
    "    bigram = tuple(sentence[-1:] + [random_word])\n",
    "    prob_bigram = kn_lm.get_log_prob_kn(bigram)\n",
    "    #print(\"bigram={}, prob={}\".format(bigram,prob_bigram))\n",
    "    random_number = uniform(0,1)\n",
    "    if(random_number < prob_bigram):\n",
    "        sentence = sentence + [random_word]\n",
    "' '.join(sentence[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = ['a','b','c']\n",
    "b = ['d']\n",
    "a[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.042908161111036014"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_lm.get_log_prob_kn(('who','is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09084592740529628"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kn_lm.get_log_prob_kn(('what','is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
