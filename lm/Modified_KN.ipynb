{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import dill as pickle\n",
    "import os\n",
    "from random import shuffle\n",
    "from random import randint\n",
    "from random import uniform\n",
    "from math import floor\n",
    "import shutil\n",
    "import codecs\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import words\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "#nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Brown corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#brown train and test\n",
    "#TRAIN_PATH = '../data/brown/train/train_and_validate.txt'\n",
    "#TEST_PATH = '../data/brown/test/test.txt'\n",
    "\n",
    "#gutenberg train and test\n",
    "#TRAIN_PATH = '../data/gutenberg/train/train_and_validate.txt'\n",
    "#TEST_PATH = '../data/gutenberg/test/test.txt'\n",
    "\n",
    "#both train, brown test\n",
    "#TRAIN_PATH = '../data/both_train/both_train.txt'\n",
    "#TEST_PATH = '../data/brown/test/test.txt'\n",
    "\n",
    "#both train, gutenberg test\n",
    "TRAIN_PATH = '../data/both_train/both_train.txt'\n",
    "TEST_PATH = '../data/gutenberg/test/test.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified KN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Modified_kn:\n",
    "    \n",
    "    def __init__(self,order):\n",
    "        self.order = order\n",
    "        self.ngram_counters = {}\n",
    "        #self.unique_continuations_ge_1 = defaultdict(int) \n",
    "        #self.unique_continuations_e_1 = defaultdict(int) \n",
    "        #self.unique_continuations_e_2 = defaultdict(int) \n",
    "        #self.unique_continuations_ge_3 = defaultdict(int) \n",
    "        #self.unique_contexts_ge_1 = defaultdict(int) \n",
    "        \n",
    "        #self.unique_continuations = {'ge1':{}, 'e1':{}, 'e2':{}, 'ge3':{}}\n",
    "        #self.unique_contexts = {'ge1':{}}\n",
    "        \n",
    "        self.unique_continuations = {}\n",
    "        self.unique_contexts = {}\n",
    "        self.D = {}\n",
    "        for i in range(1,order):\n",
    "            self.unique_continuations[i] = {'ge1':{}, 'e1':{}, 'e2':{}, 'ge3':{}}\n",
    "            self.unique_contexts[i] = {'ge1':{}}\n",
    "        for i in range(order):    \n",
    "            self.D[i+1] = {1:0.0,2:0.0,3:0.0}\n",
    "        self.n1plus_dot_ngram_dot = {}\n",
    "    \n",
    "    def set_training_data(self,train_file):\n",
    "        print(\"Setting training data\")\n",
    "        self.train_file = train_file\n",
    "        f = codecs.open(self.train_file, encoding='utf-8')\n",
    "        tokens = nltk.word_tokenize(f.read())\n",
    "        f.close()\n",
    "        tokens_original = tokens\n",
    "        unigram_counter = Counter(ngrams(tokens,1))\n",
    "        num_unk = 0\n",
    "        for i in range(len(tokens_original)):\n",
    "            if(  unigram_counter[tuple([tokens_original[i]])] < 2):  # <1 is bad\n",
    "                tokens[i] = '<unk>'\n",
    "                num_unk = num_unk + 1\n",
    "        print(\"Num unk={}\".format(num_unk))\n",
    "        self.tokens = [\"<pad>\"]*max((self.order)-1,0) + tokens\n",
    "        self.vocabulary = set(self.tokens)\n",
    "        self.vocabulary_list = list(self.vocabulary)\n",
    "        print(\"Tokens + padding ={}\".format(len(self.tokens)))\n",
    "    \n",
    "    def compute_ngram_counts(self):\n",
    "        print(\"Computing ngram counts\")\n",
    "        for i in range(self.order):\n",
    "            self.ngram_counters[i+1] = Counter(ngrams(self.tokens,i+1))\n",
    "    \n",
    "    def compute_D(self):\n",
    "        print(\"Computing discounts\")\n",
    "        n = {}\n",
    "        for i in range(self.order):\n",
    "            n[i+1] = {1:0.0,2:0.0,3:0.0,4:0.0}\n",
    "        for i in range(self.order):\n",
    "            for j in range(4):\n",
    "                n[i+1][j+1] = Counter(self.ngram_counters[i+1].values())[j+1]\n",
    "                \n",
    "        print(\"n :{}\".format(n))\n",
    "        for i in range(1,self.order+1):\n",
    "            y = (n[i][1] / (n[i][1] + 2.0*n[i][2]))# if n[i][2]>0 else 0\n",
    "            #self.D[i][1] = 1 - ((2.0*y*(n[i][2]/n[i][1])))# if n[i][1] > 0 else 0)\n",
    "            self.D[i][1] = 1 - (2.0*(n[i][2]/(n[i][1]+ (2.0*n[i][2]))))\n",
    "            self.D[i][2] = 2 - ((3.0*y*(n[i][3]/n[i][2])))# if n[i][2] > 0 else 0)\n",
    "            self.D[i][3] = 3 - ((4.0*y*(n[i][4]/n[i][3])))# if n[i][3] > 0 else 0)\n",
    "        \n",
    "        \n",
    "    def compute_single_word_continuations_and_contexts(self):\n",
    "        print(\"Computing context and continuation counts\")\n",
    "        for n in range(1,self.order):\n",
    "            ngram_and_its_continuation_words = defaultdict(list)\n",
    "            ngram_and_its_context_words = defaultdict(list)\n",
    "            \n",
    "            for ngram in self.ngram_counters[n+1].keys():\n",
    "                ngram_and_its_continuation_words[ngram[:len(ngram)-1]].append(ngram[-1:])\n",
    "                ngram_and_its_context_words[ngram[1:]].append(ngram[0:1])\n",
    "            \n",
    "            for ngram in ngram_and_its_continuation_words.keys():\n",
    "                continuation_counts_of_each_word = Counter(ngram_and_its_continuation_words[ngram])\n",
    "                number_of_each_continuation_count = Counter(continuation_counts_of_each_word.values())\n",
    "                continuations_ge1 = len(ngram_and_its_continuation_words[ngram])\n",
    "                continuations_e1 = number_of_each_continuation_count[1]\n",
    "                continuations_e2 = number_of_each_continuation_count[2]\n",
    "                continuations_ge3 = len([i for i in number_of_each_continuation_count.values() if i>=3])\n",
    "                \n",
    "                self.unique_continuations[n]['ge1'][ngram] = continuations_ge1\n",
    "                self.unique_continuations[n]['e1'][ngram] = continuations_e1\n",
    "                self.unique_continuations[n]['e2'][ngram] = continuations_e2\n",
    "                self.unique_continuations[n]['ge3'][ngram] = continuations_ge3\n",
    "                \n",
    "            for ngram in ngram_and_its_context_words.keys():\n",
    "                context_ge1 = len(set(ngram_and_its_context_words[ngram]))\n",
    "                self.unique_contexts[n]['ge1'][ngram] = context_ge1\n",
    "        for i in range(1,self.order):\n",
    "            token = tuple(['<pad>'] * i)\n",
    "            self.unique_contexts[i]['ge1'][(token)] = 1\n",
    "    \n",
    "    def compute_n1plus_dot_ngram_dot(self):\n",
    "        print(\"Computing 'that' count ... i dont know what to call it\")\n",
    "        used_continuations = defaultdict(set)\n",
    "        dot_ngram_dot_counts = defaultdict(int)\n",
    "        for n in range(2,self.order+1):\n",
    "            for ngram in self.ngram_counters[n]:\n",
    "                mid = ngram[1:-1]\n",
    "                last = ngram[-1]\n",
    "                if(last not in used_continuations[mid]):\n",
    "                    to_count = ngram[1:]\n",
    "                    count = self.unique_contexts[len(to_count)]['ge1'][to_count]\n",
    "                    dot_ngram_dot_counts[mid] = dot_ngram_dot_counts[mid] + count\n",
    "                    used_continuations[mid].add(last)\n",
    "        self.n1plus_dot_ngram_dot = dot_ngram_dot_counts\n",
    "                \n",
    "    def get_D(self,c,ngram):\n",
    "        if(c == 0):\n",
    "            return 0\n",
    "        if(c == 1):\n",
    "            return self.D[len(ngram)][1]\n",
    "        if(c == 2):\n",
    "            return self.D[len(ngram)][2]\n",
    "        if(c >= 3):\n",
    "            return self.D[len(ngram)][3]\n",
    "    \n",
    "    def train(self):\n",
    "        print(\"Training now\")\n",
    "        self.compute_ngram_counts()\n",
    "        self.compute_D()\n",
    "        self.compute_single_word_continuations_and_contexts()\n",
    "        self.compute_n1plus_dot_ngram_dot()\n",
    "        print(\"done\")\n",
    "    \n",
    "    def __preproc_test_input(self,tokens):\n",
    "        '''converts words not in vocab to <unk>'''\n",
    "        print(\"preprocessing test data\")\n",
    "        unk_count = 0\n",
    "        for i in range(len(tokens)):\n",
    "            if(tokens[i] not in  self.vocabulary):\n",
    "                tokens[i] = '<unk>'\n",
    "                unk_count = unk_count + 1\n",
    "        tokens = ['<pad>']*max((self.order)-1,0) + tokens\n",
    "        print(\"data length={}, number of <unk>={}\".format(len(tokens),unk_count))\n",
    "        return tokens\n",
    "    \n",
    "    def gamma(self, ngram):\n",
    "        t1 = self.get_D(1,ngram) * self.unique_continuations[len(ngram)]['e1'].get(ngram,0)\n",
    "        t2 = self.get_D(2,ngram) * self.unique_continuations[len(ngram)]['e2'].get(ngram,0)\n",
    "        t3 = self.get_D(3,ngram) * self.unique_continuations[len(ngram)]['ge3'].get(ngram,0)\n",
    "        #print(\"gamma:: {},{},{}\".format(t1,t2,t3))\n",
    "        t4 = 0\n",
    "        if((len(ngram) == self.order) or (len(ngram) == self.order-1)):\n",
    "            t4 = self.unique_contexts[len(ngram)]['ge1'].get(ngram,0)\n",
    "            #if t4 == 0:\n",
    "            #    print(\"gamma HIT,t4={},ngram={}\".format(t4,ngram)) \n",
    "        else:\n",
    "            t4 = self.n1plus_dot_ngram_dot[ngram]\n",
    "            #if t4 == 0:\n",
    "            #    print(\"gamma hit,t4={},ngram={}\".format(t4,ngram)) \n",
    "            \n",
    "        val = 1\n",
    "        if(t1+t2+t3 > 0 and t4!=0):\n",
    "            val = (t1+t2+t3)/float(t4)\n",
    "        return val\n",
    "    \n",
    "    def get_prob(self,ngram):\n",
    "        #print(\"ngram={}\".format(ngram))\n",
    "        if(len(ngram) == 1):\n",
    "            val = self.unique_contexts[1]['ge1'][ngram] / self.n1plus_dot_ngram_dot[()]\n",
    "            #print(\"get_prob:unigram prob for {} is {}\".format(ngram,val))\n",
    "        elif((len(ngram) == self.order) or (ngram[:self.order-1] == (['<pad>']*(self.order-1) ))):\n",
    "            t1 = self.ngram_counters[len(ngram)].get(ngram,0)\n",
    "            t2 = self.get_D(self.ngram_counters[len(ngram)].get(ngram,0) , ngram)\n",
    "            t3 = max((t1-t2),0)\n",
    "            #if(t1-t2<=0):\n",
    "            #    print(\"t1-t2<=0, ngram={} , t1={}, t2={}\".format(ngram,t1,t2))\n",
    "            t4 = self.ngram_counters[len(ngram[:-1])][ngram[:-1]]\n",
    "            t5 = (t3/float(t4)) if t4!=0 else 0\n",
    "            t6 = self.gamma(ngram[:-1])\n",
    "            #if(t6 <= 0):\n",
    "            #    print(\"gamma for {} is <=0\".format(ngram[:-1]))\n",
    "            t7 = self.get_prob(ngram[1:])\n",
    "            #if(t6 == 0):\n",
    "            #    print(\"get_prob HIT\")\n",
    "            #    return t7\n",
    "            val = t5 + t6*t7\n",
    "        else:\n",
    "            t1 = self.unique_contexts[len(ngram)]['ge1'].get(ngram,0)\n",
    "            t2 = self.get_D(self.ngram_counters[len(ngram)].get(ngram,0) , ngram)\n",
    "            t3 = max((t1-t2),0)\n",
    "            #if(t1-t2<=0):\n",
    "            #    print(\"t1-t2<=0, ngram={} , t1={}, t2={}\".format(ngram,t1,t2))\n",
    "            t4 = self.n1plus_dot_ngram_dot[ngram[1:-1]]\n",
    "            #if(t4 == 0):\n",
    "            #    print(ngram[1:-1])\n",
    "            t5 = (float(t3)/t4) if t4!=0 else 0\n",
    "            t6 = self.gamma(ngram[:-1])\n",
    "            #if(t6 <= 0):\n",
    "            #    print(\"gamma for {} is <=0\".format(ngram[:-1]))\n",
    "            t7 = self.get_prob(ngram[1:])\n",
    "            #if(t6 == 0):\n",
    "            #    print(\"get_prob hit\")\n",
    "            #    return t7\n",
    "            val = t5 + t6*t7\n",
    "        #if(val ==0 ):\n",
    "        #    print(\"ngram={}\".format(ngram))\n",
    "        return val\n",
    "\n",
    "    \n",
    "    def get_perplexity(self,test_file):\n",
    "        f = codecs.open(test_file, encoding='utf-8')\n",
    "        tokens = nltk.word_tokenize(f.read())\n",
    "        tokens = self.__preproc_test_input(tokens)\n",
    "        f.close()\n",
    "        n_grams = ngrams(tokens,self.order)\n",
    "        sum_log_prob = 0.0\n",
    "        print(\"Calculating perplexity\")\n",
    "        iter_count = 0\n",
    "        for ngram in n_grams: \n",
    "            value = self.get_prob(ngram)\n",
    "            if(value == 0):\n",
    "                print(\"get_perplexity::ngram={}\".format(ngram))\n",
    "                return math.nan\n",
    "            sum_log_prob = sum_log_prob + math.log(value)\n",
    "            iter_count += 1\n",
    "        print(\"num iterations={}\".format(iter_count))\n",
    "        return math.exp( -(1.0/len(tokens)) * sum_log_prob )\n",
    "    \n",
    "    def get_random_word(self):\n",
    "        index = randint(0,len(self.vocabulary_list)-1)\n",
    "        return self.vocabulary_list[index]\n",
    "    \n",
    "    def generate_text(self,num_tokens=20):\n",
    "        #sentence = ['<pad>'] * (self.order-1) \n",
    "        sentence = ['The']\n",
    "        while(len(sentence) < num_tokens+1):\n",
    "            try:\n",
    "                random_word = self.get_random_word()\n",
    "                if(random_word == '<unk>'):\n",
    "                    continue\n",
    "                ngram = tuple(sentence[-self.order+1:] + [random_word])\n",
    "                prob_ngram = self.get_prob(ngram)\n",
    "                #print(\"bigram={}, prob={}\".format(bigram,prob_bigram))\n",
    "                random_number = uniform(0,1)\n",
    "                if(random_number < prob_ngram):\n",
    "                    sentence = sentence + [random_word]\n",
    "                    print(sentence)\n",
    "            except:\n",
    "            #    continue\n",
    "                raise \n",
    "        return ' '.join(sentence)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = Modified_kn(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting training data\n",
      "Num unk=32770\n",
      "Tokens + padding =2731302\n"
     ]
    }
   ],
   "source": [
    "lm.set_training_data(TRAIN_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training now\n",
      "Computing ngram counts\n",
      "Computing discounts\n",
      "n :{1: {1: 0, 2: 9427, 3: 5236, 4: 3382}, 2: {1: 577745, 2: 99142, 3: 39509, 4: 21465}, 3: {1: 1655201, 2: 134417, 3: 42362, 4: 19663}, 4: {1: 2320380, 2: 86357, 3: 20157, 4: 8205}}\n",
      "Computing context and continuation counts\n",
      "Computing 'that' count ... i dont know what to call it\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00026433308359387754"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_prob(('who','is'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing test data\n",
      "data length=276979, number of <unk>=7984\n",
      "Calculating perplexity\n",
      "num iterations=276976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13905.56346081903"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.get_perplexity(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'nine']\n",
      "['The', 'nine', 'isnt']\n",
      "['The', 'nine', 'isnt', 'good']\n",
      "['The', 'nine', 'isnt', 'good', 'looks']\n",
      "['The', 'nine', 'isnt', 'good', 'looks', 'concentration']\n",
      "['The', 'nine', 'isnt', 'good', 'looks', 'concentration', 'begat']\n",
      "['The', 'nine', 'isnt', 'good', 'looks', 'concentration', 'begat', 'sweet']\n",
      "['The', 'nine', 'isnt', 'good', 'looks', 'concentration', 'begat', 'sweet', 'sovran']\n",
      "['The', 'nine', 'isnt', 'good', 'looks', 'concentration', 'begat', 'sweet', 'sovran', 'curls']\n",
      "['The', 'nine', 'isnt', 'good', 'looks', 'concentration', 'begat', 'sweet', 'sovran', 'curls', 'continually']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The nine isnt good looks concentration begat sweet sovran curls continually'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm.generate_text(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
